{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Horse Race Prediction\n",
    "## Regression Modelling\n",
    "- In this section, we want to predict the finishing times of horses in a race, and then use it to predict the winner.\n",
    "- We will use RMSE to evaluate, then after classification of the horse with the fastest time, find the accuracy of our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test files\n",
    "df_train1 = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\df_train.csv')\n",
    "df_test1 = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\df_test.csv')\n",
    "df_unseen1 = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\df_unseen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train1#.dropna()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test1#.dropna()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen = df_unseen1#.dropna()\n",
    "df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(0, inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen.fillna(0, inplace=True)\n",
    "df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the shape of the train and test files\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_unseen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the train file\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the test files\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the unseen file\n",
    "df_unseen.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['win_odds'] = df_test['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "df_train['win_odds'] = df_train['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "df_unseen['win_odds'] =df_unseen['win_odds'].str.rstrip('%').astype('float') / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from numba import jit, cuda\n",
    "\n",
    "@jit(target_backend='cuda')\n",
    "def convert_to_km(distance):\n",
    "    '''\n",
    "    distance can be a string with km or m as units\n",
    "    e.g. 300km, 1.1km, 200m, 4.5m\n",
    "    '''\n",
    "    \n",
    "    # split the string into value and unit ['300', 'km']\n",
    "    #split_dist = re.match('([\\d\\.]+)?([a-zA-Z]+)', distance)\n",
    "    split_dist = re.findall('([\\d\\.]+)?([a-zA-Z]+)', distance, re.U)\n",
    "    print(split_dist)\n",
    "    dist = 0.0\n",
    "    dist1 = 0.0\n",
    "    dist2 = 0.0\n",
    "    dist3 = 0.0\n",
    "    for value in split_dist:\n",
    "        if value[1] == 'm':\n",
    "            dist1 = float(value[0])\n",
    "        elif value[1] == 'f':\n",
    "            dist2 = float(value[0])*0.125\n",
    "        elif value[1] == 'y':\n",
    "            dist3 = float(value[0])*0.0005681818\n",
    "        else:\n",
    "            pass\n",
    "      \n",
    "    dist = dist1 + dist2 + dist3\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['race_distance'] = df_train.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['race_distance'] = df_test.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen['race_distance'] = df_unseen.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_unseen.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[[#'actual_weight', \n",
    "                    'declared_horse_weight',\n",
    "                    'draw', \n",
    "                    'win_odds',\n",
    "                    'jockey_ave_rank','trainer_ave_rank',\n",
    "                    'recent_ave_rank','race_distance', \n",
    "                     'jockey_hurdle_rate','trainers_hurdle_rate']]\n",
    "\n",
    "# Define the target\n",
    "y_train = df_train['jockey_ave_rank']\n",
    "\n",
    "# Convert the target to seconds\n",
    "#y_train = y_train.apply(lambda x: x.split('.'))\n",
    "#y_train = y_train.apply(lambda x: int(x[0])*60 + int(x[1]) + int(x[2])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing set\n",
    "X_test = df_test[[#'actual_weight', \n",
    "                  'declared_horse_weight',\n",
    "                    'draw', \n",
    "                     'win_odds', \n",
    "                    'jockey_ave_rank', 'trainer_ave_rank',\n",
    "                    'recent_ave_rank', 'race_distance', \n",
    "                     'jockey_hurdle_rate','trainers_hurdle_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target\n",
    "y_test = df_test['jockey_ave_rank']\n",
    "\n",
    "# Convert the target to seconds\n",
    "#y_test = y_test.apply(lambda x: x.split('.'))\n",
    "#y_test = y_test.apply(lambda x: int(x[0])*60 + int(x[1]) + int(x[2])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unseen set\n",
    "X_unseen = df_unseen[[#'actual_weight', \n",
    "                      'declared_horse_weight',\n",
    "                    'draw', \n",
    "                     'win_odds',\n",
    "                    'jockey_ave_rank', 'trainer_ave_rank',\n",
    "                    'recent_ave_rank', 'race_distance', \n",
    "                     'jockey_hurdle_rate','trainers_hurdle_rate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to run and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the accuracy of the model for predicting the Top and Top 3 finishers\n",
    "def find_prob(y_pred):\n",
    "    \n",
    "    i=0\n",
    "    count_top_winners = 0\n",
    "    count_top_correct = 0\n",
    "\n",
    "    count_top3_winners = 0\n",
    "    count_top3_correct = 0\n",
    "\n",
    "    for column in ['HorseWin', 'HorseRankTop3']:\n",
    "            \n",
    "        for race in df_test['race_id'].unique():\n",
    "            \n",
    "            # Create temp dataframe\n",
    "            temp = df_test[df_test['race_id']==race]\n",
    "\n",
    "            # Get the index of the temp dataframe\n",
    "            temp_index = temp.index\n",
    "\n",
    "            # Find the index of the winners from the temp dataframe\n",
    "            if i == 0:\n",
    "                winners_index = temp[temp['finishing_position']==1].index\n",
    "            else:\n",
    "                winners_index = temp[temp['finishing_position']<=3].index\n",
    "\n",
    "            # Create a temp dataframe for the predicted probabilities\n",
    "            temp_pred = y_pred.iloc[temp_index]\n",
    "\n",
    "            # Sort the temp dataframe by the predicted timings\n",
    "            temp_pred = temp_pred.sort_values(by=temp_pred.columns[0])\n",
    "\n",
    "            # Get the index of the winners from the temp pred dataframe\n",
    "            if i == 0:\n",
    "                winners_pred_index = temp_pred[:1].index\n",
    "            else:\n",
    "                winners_pred_index = temp_pred[:3].index\n",
    "\n",
    "            # Count the number of winners and correct predictions\n",
    "            if i == 0:\n",
    "                count_top_winners += len(winners_index)\n",
    "                count_top_correct += len(set(winners_index).intersection(set(winners_pred_index)))\n",
    "            else:\n",
    "                count_top3_winners += len(winners_index)\n",
    "                count_top3_correct += len(set(winners_index).intersection(set(winners_pred_index)))\n",
    "        i+=1\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    top_accuracy = round(count_top_correct/count_top_winners, 3)\n",
    "    top3_accuracy = round(count_top3_correct/count_top3_winners, 3)\n",
    "\n",
    "    return top_accuracy, top3_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the results\n",
    "results = pd.DataFrame(columns=['Model', 'RMSE_train', 'RMSE_test', \n",
    "                                'Generalization', 'Top1_Train_Accuracy', 'Top1_Test_Accuracy',\n",
    "                                'Top3_Train_Accuracy', 'Top3_Test_Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run the model\n",
    "def run_model(model, X_train, y_train, X_test, y_test, X_unseen):\n",
    "\n",
    "        # Store model name\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_unseen = scaler.transform(X_unseen)\n",
    "\n",
    "        # Fit the model         \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_pred = pd.DataFrame(y_train_pred)\n",
    "\n",
    "        # Predict on the testing set\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred = pd.DataFrame(y_test_pred)\n",
    "\n",
    "        # Calculate the RMSE\n",
    "        train_rmse = round(math.sqrt(mean_squared_error(y_train, y_train_pred)), 3)\n",
    "        test_rmse = round(math.sqrt(mean_squared_error(y_test, y_test_pred)), 3)\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        train_accuracy, train_accuracy_top3 = find_prob(y_train_pred)\n",
    "        test_accuracy, test_accuracy_top3 = find_prob(y_test_pred)\n",
    "\n",
    "        # Calculate generalization error percentage\n",
    "        generalization_error = round((test_rmse - train_rmse)/train_rmse*100, 3)\n",
    "\n",
    "        # Print the results\n",
    "        print('Model results for', model_name, ':')\n",
    "        print('Train RMSE: ', train_rmse)\n",
    "        print('Test RMSE: ', test_rmse)\n",
    "        print('Generalization Error: ', generalization_error, '%', '\\n')\n",
    "\n",
    "        print('Train Accuracy for finding Top position: ', train_accuracy)\n",
    "        print('Test Accuracy for finding Top position: ', test_accuracy, '\\n')\n",
    "\n",
    "        print('Train Accuracy for finding Top 3 positions: ', train_accuracy_top3)\n",
    "        print('Test Accuracy for finding Top 3 positions: ', test_accuracy_top3)\n",
    "\n",
    "        # Append the results to the dataframe\n",
    "        results.loc[len(results)] = [model_name, train_rmse, test_rmse, generalization_error,\n",
    "                                  train_accuracy, test_accuracy, train_accuracy_top3, test_accuracy_top3]\n",
    "        \n",
    "        # predict on unseen data\n",
    "        y_unseen_pred = model.predict(X_unseen)\n",
    "        y_unseen_pred = pd.DataFrame(y_unseen_pred)\n",
    "\n",
    "        return y_unseen_pred\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "number = len(X_train)\n",
    "print(number)\n",
    "alpha_range= np.arange(0,1000)\n",
    "ridge = Ridge(alpha=60396, solver='cholesky')\n",
    "#ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "ridge_pred = run_model(ridge, X_train, y_train, X_test, y_test, X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsRegressor(n_neighbors=500)\n",
    "\n",
    "knn_pred = run_model(knn, X_train, y_train, X_test, y_test, X_unseen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "rf = RandomForestRegressor(n_estimators=30, max_depth=4, random_state=42, max_features=5,\n",
    "                            min_samples_split=20, min_samples_leaf=200, n_jobs=-1)\n",
    "\n",
    "rf_pred = run_model(rf, X_train, y_train, X_test, y_test, X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Light Gradient Boosting Machine (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "lgbm = LGBMRegressor(n_estimators=20, max_depth=5, random_state=42, num_leaves=100,\n",
    "                     min_child_samples=10, min_child_weight=10, n_jobs=-1)\n",
    "\n",
    "lgbm_pred = run_model(lgbm, X_train, y_train, X_test, y_test, X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results of our 4 regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objective is to have low RMSE, good generalisation, and good training accuracy, the LGBMRegressor meets all the criteria and we will choose it as our final model for backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "ridge_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\ridge_pred.csv')\n",
    "knn_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\knn_pred.csv')\n",
    "rf_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\rf_pred.csv')\n",
    "lgbm_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\lgbm_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(lgbm, 'D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "results.to_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\reg_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0544f3d8f88c277ece8a7bca9c4cae84dfc7832020b3e571294385a87fe1e90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
