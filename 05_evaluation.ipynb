{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Horse Race Prediction\n",
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test files\n",
    "df_train = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\df_train.csv')\n",
    "df_test = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(0, inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the shape of the train and test files\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from numba import jit, cuda\n",
    "\n",
    "@jit(target_backend='cuda')\n",
    "def convert_to_km(distance):\n",
    "    '''\n",
    "    distance can be a string with km or m as units\n",
    "    e.g. 300km, 1.1km, 200m, 4.5m\n",
    "    '''\n",
    "    \n",
    "    # split the string into value and unit ['300', 'km']\n",
    "    #split_dist = re.match('([\\d\\.]+)?([a-zA-Z]+)', distance)\n",
    "    split_dist = re.findall('([\\d\\.]+)?([a-zA-Z]+)', distance, re.U)\n",
    "    print(split_dist)\n",
    "    dist = 0.0\n",
    "    dist1 = 0.0\n",
    "    dist2 = 0.0\n",
    "    dist3 = 0.0\n",
    "    for value in split_dist:\n",
    "        if value[1] == 'm':\n",
    "            dist1 = float(value[0])\n",
    "        elif value[1] == 'f':\n",
    "            dist2 = float(value[0])*0.125\n",
    "        elif value[1] == 'y':\n",
    "            dist3 = float(value[0])*0.0005681818\n",
    "        else:\n",
    "            pass\n",
    "      \n",
    "    dist = dist1 + dist2 + dist3\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['race_distance'] = df_train.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['race_distance'] = df_test.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['win_odds'] = df_test['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "df_train['win_odds'] = df_train['win_odds'].str.rstrip('%').astype('float') / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the train file\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the test files\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the features we want to train our model on\n",
    "X_train = df_train[[#'actual_weight', \n",
    "                    'declared_horse_weight',\n",
    "                    'draw', \n",
    "                    'win_odds', \n",
    "                    'jockey_ave_rank',\n",
    "                    'trainer_ave_rank', 'recent_ave_rank', 'race_distance', \n",
    "                    'jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate']]\n",
    "\n",
    "y_train = df_train['HorseWin']\n",
    "\n",
    "# Keep the features we want to train our model on\n",
    "X_test = df_test[[#'actual_weight', \n",
    "                  'declared_horse_weight',\n",
    "                   'draw', \n",
    "                    'win_odds',\n",
    "                   'jockey_ave_rank',\n",
    "                    'trainer_ave_rank', 'recent_ave_rank','race_distance', \n",
    "                    'jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate']]\n",
    "                    \n",
    "y_test = df_test[['HorseWin', 'HorseRankTop3']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickled models\n",
    "smote_rfc = joblib.load('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\smote_rfc_model.pkl')\n",
    "lgbm = joblib.load('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature importance with random forest\n",
    "def feature_importance(model, X_train, y_train):\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # get importance\n",
    "    importance = model[1][1].feature_importances_\n",
    "\n",
    "    # sort the index of the importance\n",
    "    sorted_idx = np.argsort(importance)\n",
    "\n",
    "    # name of the features\n",
    "    feature_names = X_train.columns[sorted_idx]\n",
    "\n",
    "    # summarize feature importance with feature names\n",
    "    for i,v in enumerate(importance[sorted_idx]):\n",
    "        print('Feature: %s, Score: %.5f' % (feature_names[i],v))\n",
    "\n",
    "    # plot a barh graph of feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_names, importance[sorted_idx])\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature importance\n",
    "feature_importance(smote_rfc, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values\n",
    "import shap\n",
    "\n",
    "# fit the model\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.Explainer(lgbm.predict, X_test)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the SHAP values\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bee swarm plot is used to understand the importance or contribution of features for the whole dataset. Looking at the Recent Average Rank variable, low values (in blue) has a very high contribution towards the prediction. All the little dots represent a single observation. The horizontal axis represents the SHAP value.\n",
    "\n",
    "All variables are shown in order of global feature importance, the first one being most important and last one being least important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the SHAP values\n",
    "shap.plots.bar(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bar plot, the features are ordered from the highest to lowest effect on prediction. It takes in account the absolute SHAP value, so it does not matter if the feature affects the prediction in a positive or negative way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the SHAP values\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This local plot (a waterfall plot) shows what are the main features affecting the prediction of a single observation, and the magnitude of the SHAP value for each feature. Here we can see how the sum of all SHAP values equals the difference between the prediction and the expected value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0544f3d8f88c277ece8a7bca9c4cae84dfc7832020b3e571294385a87fe1e90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
