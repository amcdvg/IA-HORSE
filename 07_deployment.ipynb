{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdc35aa",
   "metadata": {},
   "source": [
    "# Part 7: Deployment\n",
    "- This section aims to simplify the machine learning model into a simple and usable app that punters can use to decide whether to make a bet or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the train file\n",
    "df_train = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\df_train.csv')\n",
    "#df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Read in the test file\n",
    "df_test = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\df_test.csv')\n",
    "#df_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(0, inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd26575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['win_odds'] = df_test['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "df_train['win_odds'] = df_train['win_odds'].str.rstrip('%').astype('float') / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffaf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from numba import jit, cuda\n",
    "\n",
    "@jit(target_backend='cuda')\n",
    "def convert_to_km(distance):\n",
    "    '''\n",
    "    distance can be a string with km or m as units\n",
    "    e.g. 300km, 1.1km, 200m, 4.5m\n",
    "    '''\n",
    "    \n",
    "    # split the string into value and unit ['300', 'km']\n",
    "    #split_dist = re.match('([\\d\\.]+)?([a-zA-Z]+)', distance)\n",
    "    split_dist = re.findall('([\\d\\.]+)?([a-zA-Z]+)', distance, re.U)\n",
    "    print(split_dist)\n",
    "    dist = 0.0\n",
    "    dist1 = 0.0\n",
    "    dist2 = 0.0\n",
    "    dist3 = 0.0\n",
    "    for value in split_dist:\n",
    "        if value[1] == 'm':\n",
    "            dist1 = float(value[0])\n",
    "        elif value[1] == 'f':\n",
    "            dist2 = float(value[0])*0.125\n",
    "        elif value[1] == 'y':\n",
    "            dist3 = float(value[0])*0.0005681818\n",
    "        else:\n",
    "            pass\n",
    "      \n",
    "    dist = dist1 + dist2 + dist3\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['race_distance'] = df_train.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['race_distance'] = df_test.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8949fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the features we want to train our model on\n",
    "df = df_train[['horse_name', 'horse_rate', 'jockey',\n",
    "                #'actual_weight', \n",
    "               'declared_horse_weight',\n",
    "                    'draw', 'win_odds', 'jockey_ave_rank',\n",
    "                     'recent_ave_rank', 'race_distance', 'HorseWin', 'HorseRankTop3', \n",
    "                    'jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate']]\n",
    "\n",
    "dftest = df_test[['horse_name', 'horse_rate', 'jockey',\n",
    "                # 'actual_weight', \n",
    "                  'declared_horse_weight',\n",
    "                'draw', 'win_odds', 'jockey_ave_rank',    \n",
    "                 'recent_ave_rank', 'race_distance', 'HorseWin', 'HorseRankTop3',\n",
    "                 'jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796597d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the below code if we want to encode certain features, else we dont have to use this.\n",
    "# # instantiate labelencoder object\n",
    "# le_horse_name = LabelEncoder()\n",
    "\n",
    "# # apply le on categorical feature columns\n",
    "# df['horse_name'] = le_horse_name.fit_transform(df['horse_name'])\n",
    "\n",
    "# # instantiate labelencoder object\n",
    "# le_jockey = LabelEncoder()\n",
    "\n",
    "# # apply le on categorical feature columns\n",
    "# df['jockey'] = le_jockey.fit_transform(df['jockey'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d645d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After encoding, we can see that the horse_name and jockey columns are now integers\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c698ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split df into X and y\n",
    "X_train = df.drop(['horse_name','jockey', 'horse_rate', 'jockey', 'HorseWin', 'HorseRankTop3','draw','jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate'], axis=1)\n",
    "y_train = df['HorseWin']\n",
    "\n",
    "# split dftest into X and y\n",
    "X_test = dftest.drop(['horse_name', 'jockey', 'horse_rate', 'jockey', 'HorseWin', 'HorseRankTop3','draw','jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate'], axis=1)\n",
    "y_test = dftest['HorseWin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote the training data\n",
    "sm = SMOTE(random_state = 42)\n",
    "rfc = RandomForestClassifier(max_depth=20, min_samples_leaf=10,\n",
    "                               random_state = 42)\n",
    "\n",
    "# Steps for the pipeline\n",
    "steps = [('smote', sm), ('rfc', rfc)]\n",
    "\n",
    "# Create the pipeline\n",
    "smote_rfc = Pipeline(steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "smote_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3e71f",
   "metadata": {},
   "source": [
    "### Checking if the model is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96682ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify kfold cross validation\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Calculate the cross validation score\n",
    "cv_score = cross_val_score(smote_rfc, X_train, y_train, cv=kfold, scoring='f1_weighted').mean()\n",
    "cv_score = round(cv_score, 3)\n",
    "\n",
    "# Create a dataframe to store the predictions\n",
    "df_pred = pd.DataFrame()\n",
    "df_pred['RaceID'] = df_test['race_id']\n",
    "df_pred['HorseID'] = df_test['horse_id']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = smote_rfc.predict(X_test)\n",
    "\n",
    "# Store the predictions in the dataframe\n",
    "df_pred['HorseWin'] = y_test_pred\n",
    "\n",
    "# Calculate the f1 score\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = round(f1, 3)\n",
    "\n",
    "# Calculate PR AUC\n",
    "pr_auc = average_precision_score(y_test, y_test_pred, average='weighted')\n",
    "pr_auc = round(pr_auc, 3)\n",
    "\n",
    "# Calculate TPR\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "tpr = tp / (tp + fn)\n",
    "tpr = round(tpr, 3)\n",
    "\n",
    "\n",
    " # Print the results\n",
    "print('Cross Validation Score: ', cv_score)\n",
    "print('F1 Score: ', f1)\n",
    "print('PR AUC (Avg Precision): ', pr_auc)\n",
    "print('TPR: ', tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to backtest the betting strategy\n",
    "def simple_class_strategy(model_pred, graph=True):\n",
    "\n",
    "    df_test_results = df_test[['finishing_position', 'win_odds', \n",
    "                                'race_id', \n",
    "                               'HorseWin', 'horse_id']]\n",
    "\n",
    "    # rename columns\n",
    "    df_test_results = df_test_results.rename(columns={'race_id': 'RaceID', \n",
    "                                                    'horse_id': 'HorseID', \n",
    "                                                    'HorseWin':'ActualWin'})\n",
    "    \n",
    "    # merge the prediction with the test data\n",
    "    df_backtest = pd.merge(model_pred, df_test_results, on=('RaceID', 'HorseID'), how='left')\n",
    "\n",
    "    money = 0\n",
    "    bets_made = []\n",
    "    cumulative_money = [0]\n",
    "\n",
    "    for race_id in df_backtest['RaceID'].unique():\n",
    "\n",
    "        # make a temporary dataframe one for that particular race\n",
    "        temp_df = df_backtest[df_backtest['RaceID']==race_id]\n",
    "\n",
    "        # find out the bets we made\n",
    "        bets = temp_df[temp_df['HorseWin']==1]\n",
    "\n",
    "        # deduct money for bets we made\n",
    "        deduction  = -len(bets)\n",
    "\n",
    "        # amount won from bets\n",
    "        # sum of multiplying the odds with the prediction\n",
    "        amount_won = sum(bets['win_odds']*bets['ActualWin'])\n",
    "        \n",
    "        # add the amount won to the money\n",
    "        money += (amount_won + deduction)\n",
    "\n",
    "        # append the money to the cumulative money list\n",
    "        cumulative_money.append(money)\n",
    "\n",
    "        # append the bets made to the bets made list\n",
    "        bets_made.append(len(bets))\n",
    "    \n",
    "    if graph==True:\n",
    "        # plot the cumulative money\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(cumulative_money)\n",
    "        plt.title('Cumulative Money')\n",
    "        plt.xlabel\n",
    "        plt.show()\n",
    "\n",
    "        # plot the bets made\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(bets_made)\n",
    "        plt.title('Bets Made')\n",
    "        plt.show()\n",
    "\n",
    "    # print the final money and bets made\n",
    "    print('Final Money: ', round(money, 3))\n",
    "    print('Total Bets Made: ', round(sum(bets_made),3), '\\n')\n",
    "\n",
    "    return money, bets_made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62869c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the strategy works\n",
    "app_testing = simple_class_strategy(df_pred, graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to csv\n",
    "pd.DataFrame(df_pred).to_csv('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\deploy_pred.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the model\n",
    "\n",
    "# create the data to pickle\n",
    "data = {\"model\": smote_rfc, 'train_data': df_train, 'test_data': df_test}\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('D:\\\\documentos\\\\IA Caballos\\\\flat_data\\\\saved_steps.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762a311",
   "metadata": {},
   "source": [
    "### For demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to be able to show that our deployment app is working\n",
    "# Find index of df_pred where HorseWin = 1\n",
    "index = df_pred[df_pred['HorseWin']==1].index\n",
    "\n",
    "# Pass the index through to the test data\n",
    "horsesPred1 = df_test.iloc[index]\n",
    "\n",
    "# Keep only the columns we need\n",
    "horsesPredCol = ['horse_name', 'horse_rate', 'jockey', 'draw', \n",
    "                 'win_odds', 'race_distance',\n",
    "                #'actual_weight', \n",
    "                 'declared_horse_weight', 'recent_ave_rank',\n",
    "                'finishing_position', 'HorseWin', 'jockey_ave_rank', 'trainer_ave_rank','HorseRankTop3','jockey_flat_aw_rate',\n",
    "                    'jockey_flat_turf_rate',\n",
    "                    'trainers_flat_aw_rate',\n",
    "                    'trainers_flat_turf_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "horsesPred1[horsesPredCol][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b60c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_model():\n",
    "    #with open('C:\\\\Users\\\\ROOM\\Documents\\\\IA Caballos\\\\Lingfield\\\\saved_steps_lingfield_flat.pkl', 'rb') as f:\n",
    "     #   data = pickle.load(f)\n",
    "    openFile = open('saved_steps.pkl', \"rb\")\n",
    "    data = pickle.load(openFile)\n",
    "    return data\n",
    "data = load_model()\n",
    "model = data['model']\n",
    "df_test = data['test_data']\n",
    "df_train = data['train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b540acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0544f3d8f88c277ece8a7bca9c4cae84dfc7832020b3e571294385a87fe1e90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
