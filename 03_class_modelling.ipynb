{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Horse Race Prediction\n",
    "## Classification Modelling\n",
    "- In this section, we want to classify if a horse can win the race, and which three horses will be ranked in the top 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test files\n",
    "df_train1 = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\df_train.csv', index_col=0)\n",
    "df_test1 = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\df_test.csv', index_col=0)\n",
    "df_unseen1 = pd.read_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\df_unseen.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train1.dropna()\n",
    "df_train = df_train1\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = df_test1.dropna()\n",
    "df_test = df_test1\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_unseen = df_unseen1.dropna()\n",
    "df_unseen = df_unseen1\n",
    "df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(0, inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.fillna(0, inplace=True)\n",
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen.fillna(0, inplace=True)\n",
    "df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the shape of the train and test files\n",
    "print(df_train.shape)\n",
    "print(df_test1.shape)\n",
    "print(df_unseen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the train file\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the test files\n",
    "df_test1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 2 rows of the unseen file\n",
    "df_unseen.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from numba import jit, cuda\n",
    "\n",
    "@jit(target_backend='cuda')\n",
    "def convert_to_km(distance):\n",
    "    '''\n",
    "    distance can be a string with km or m as units\n",
    "    e.g. 300km, 1.1km, 200m, 4.5m\n",
    "    '''\n",
    "    \n",
    "    # split the string into value and unit ['300', 'km']\n",
    "    #split_dist = re.match('([\\d\\.]+)?([a-zA-Z]+)', distance)\n",
    "    split_dist = re.findall('([\\d\\.]+)?([a-zA-Z]+)', distance, re.U)\n",
    "    print(split_dist)\n",
    "    dist = 0.0\n",
    "    dist1 = 0.0\n",
    "    dist2 = 0.0\n",
    "    dist3 = 0.0\n",
    "    for value in split_dist:\n",
    "        if value[1] == 'm':\n",
    "            dist1 = float(value[0])\n",
    "        elif value[1] == 'f':\n",
    "            dist2 = float(value[0])*0.125\n",
    "        elif value[1] == 'y':\n",
    "            dist3 = float(value[0])*0.0005681818\n",
    "        else:\n",
    "            pass\n",
    "      \n",
    "    dist = dist1 + dist2 + dist3\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['race_distance'] = df_train.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['race_distance'] = df_test.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen['race_distance'] = df_unseen.apply(lambda row: convert_to_km(row['race_distance']), axis=1)\n",
    "\n",
    "df_unseen.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1['win_odds'] = df_test1['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "df_train['win_odds'] = df_train['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "df_unseen['win_odds'] =df_unseen['win_odds'].str.rstrip('%').astype('float') / 100.0\n",
    "#jockey_percentage_wins\n",
    "#df_test['jockey_percentage_wins'] = df_test['jockey_percentage_wins'].str.rstrip('%').astype('float') / 100.0\n",
    "#df_train['jockey_percentage_wins'] = df_train['jockey_percentage_wins'].str.rstrip('%').astype('float') / 100.0\n",
    "#df_unseen['jockey_percentage_wins'] =df_unseen['jockey_percentage_wins'].str.rstrip('%').astype('float') / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the features we want to train our model on\n",
    "X_train = df_train[[#'actual_weight', \n",
    "                    'declared_horse_weight',\n",
    "                    'draw', 'win_odds', \n",
    "                    'jockey_ave_rank',\n",
    "                    'trainer_ave_rank', \n",
    "                    'recent_ave_rank',\n",
    "                    'race_distance', \n",
    "                     'jockey_hurdle_rate','trainers_hurdle_rate']]\n",
    "\n",
    "y_train = df_train[['HorseWin', 'HorseRankTop3', 'HorseRankTop50Percent']]\n",
    "\n",
    "# Keep the features we want to train our model on\n",
    "X_test = df_test1[[#'actual_weight',\n",
    "                  'declared_horse_weight',\n",
    "                   'draw', 'win_odds',\n",
    "                   'jockey_ave_rank',\n",
    "                    'trainer_ave_rank', \n",
    "                    'recent_ave_rank', \n",
    "                    'race_distance', \n",
    "                     'jockey_hurdle_rate','trainers_hurdle_rate']]\n",
    "                    \n",
    "y_test = df_test1[['HorseWin', 'HorseRankTop3', 'HorseRankTop50Percent']]\n",
    "\n",
    "# Keep the features we want for the unseen data\n",
    "X_unseen = df_unseen[[#'actual_weight', \n",
    "                      'declared_horse_weight',\n",
    "                     'draw', 'win_odds',\n",
    "                        'jockey_ave_rank',\n",
    "                        'trainer_ave_rank', 'recent_ave_rank', \n",
    "                        'race_distance', \n",
    "                     'jockey_hurdle_rate','trainers_hurdle_rate']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the shape of the train and test files\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(df_unseen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean of the target variable\n",
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is data imbalance for the HorseWin and HorseRankTop3 variables, so we need to account for these later when modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify kfold cross validation\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to run and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the results\n",
    "df_results = pd.DataFrame(columns=['Model', 'Prediction', 'CV-F1', 'F1 Score', 'AUC', 'Recall', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run the model\n",
    "def run_model(model, X_train, y_train, X_test, y_test, kfold):\n",
    "\n",
    "    # Create a dataframe to store the predictions for the UNSEEN data\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['RaceID'] = df_unseen['race_id']\n",
    "    df_pred['HorseID'] = df_unseen['horse_id']\n",
    "\n",
    "    # Store model name\n",
    "    model_name = str(model).split('(')[0]\n",
    "\n",
    "    for column in ['HorseWin', 'HorseRankTop3']:\n",
    "\n",
    "        # Print the column name\n",
    "        print(f\"Results for model {model_name} and target variable {column}:\")\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train[column].to_numpy())\n",
    "        \n",
    "        # Calculate the cross validation score\n",
    "        cv_score = cross_val_score(model, X_train, y_train[column].to_numpy(), cv=kfold, scoring='f1_weighted').mean()\n",
    "        cv_score = round(cv_score, 3)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_unseen_pred = model.predict(X_unseen)\n",
    "\n",
    "        # Store the predictions in the dataframe\n",
    "        df_pred[column] = y_unseen_pred\n",
    "        \n",
    "        # Calculate the f1 score\n",
    "        f1 = f1_score(y_test[column].to_numpy(), y_pred, average='weighted')\n",
    "        f1 = round(f1, 3)\n",
    "        \n",
    "        # Calculate PR AUC\n",
    "        pr_auc = average_precision_score(y_test[column].to_numpy(), y_pred, average='weighted')\n",
    "        pr_auc = round(pr_auc, 3)\n",
    "\n",
    "        # Calculate Recall\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test[column].to_numpy(), y_pred).ravel()\n",
    "        recall = tp / (tp + fn)\n",
    "        recall = round(recall, 3)\n",
    "\n",
    "        # Calculate Precision\n",
    "        precision = tp / (tp + fp)\n",
    "        precision = round(precision, 3)\n",
    "\n",
    "        # Append the results to the dataframe\n",
    "        df_results.loc[len(df_results)] = [model_name, column, cv_score, f1, pr_auc, recall, precision]\n",
    "\n",
    "        # Print the results\n",
    "        print('Cross Validation Score (F1-weighted): ', cv_score)\n",
    "        print('F1 Score: ', f1)\n",
    "        print('PR AUC (Avg Precision): ', pr_auc)\n",
    "        print('Recall: ', recall)\n",
    "        print('Precision: ', precision)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(y_test, y_pred, column)\n",
    "\n",
    "        # Plot precision recall curve\n",
    "        plot_pr_auc(X_test, y_test, model, column)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot confusion matrix function\n",
    "def plot_confusion_matrix(y_test, y_pred, column):\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test[column].to_numpy(), y_pred)\n",
    "    cm = pd.DataFrame(cm, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix for ' + column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot PR AUC function\n",
    "def plot_pr_auc(X_test, y_test, model, column):\n",
    "    # Get the probabilities of the predictions\n",
    "    win_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Get the precision and recall\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test[column], win_prob)\n",
    "\n",
    "    # Calculate proportion of positive class\n",
    "    proportion_pos_class = y_test[column].mean()\n",
    "    \n",
    "    # Print the PR AUC score\n",
    "    pr_auc = round(auc(recall, precision),3)\n",
    "    print(f'PR AUC score for {column}:', pr_auc)\n",
    "\n",
    "    # Plot the PR AUC curve\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot([0, 1], [proportion_pos_class, proportion_pos_class], linestyle = '--')\n",
    "    plt.plot(recall, precision, marker = '.')\n",
    "    plt.title(f'PR AUC Curve for {column}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do hyperparameter tuning\n",
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "              'solver':['liblinear']}\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid, cv=kfold, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
    "\n",
    "# find the best parameters\n",
    "grid.fit(X_train, y_train['HorseWin'].to_numpy())\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Initialize the model using best parameters\n",
    "lr = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistics Regression model\n",
    "lr_pred = run_model(lr, X_train, y_train, X_test, y_test, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do hyperparameter tuning\n",
    "param_grid = {'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(gnb, param_grid, cv=kfold, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
    "\n",
    "# find the best parameters\n",
    "grid.fit(X_train, y_train['HorseWin'].to_numpy())\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Initialize the model using best parameters\n",
    "gnb = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Naive Bayes model\n",
    "gnb_pred = run_model(gnb, X_train, y_train, X_test, y_test, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rfc = RandomForestClassifier(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do hyperparameter tuning\n",
    "param_grid = {'max_depth': [20, 40],\n",
    "                'max_features': [5, 10],\n",
    "                'min_samples_leaf': [1, 2],\n",
    "                'min_samples_split': [1, 2]}\n",
    "grid = GridSearchCV(rfc, param_grid, cv=kfold, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
    "\n",
    "# find the best parameters\n",
    "grid.fit(X_train, y_train['HorseWin'].to_numpy())\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Initialize the model using best parameters\n",
    "rfc = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Random Forest model\n",
    "rfc_pred = run_model(rfc, X_train, y_train, X_test, y_test, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: SMOTE + Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote the training data\n",
    "sm = SMOTE(random_state = 42)\n",
    "rfc = RandomForestClassifier(max_depth=10, random_state = 42)\n",
    "\n",
    "# Steps for the pipeline\n",
    "steps = [('smote', sm), ('rfc', rfc)]\n",
    "\n",
    "# Create the pipeline\n",
    "smote_rfc = Pipeline(steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "smote_rfc_pred = run_model(smote_rfc, X_train, y_train, X_test, y_test, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the final results of our 4 classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results\n",
    "df_results.sort_values(by='Prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the Random Forest Classifer as our deployed model as it has good generalisation between the train and test F1 scores, and the highest PR AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our models to csv\n",
    "lr_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\lr_pred.csv', index=False)\n",
    "gnb_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\gnb_pred.csv', index=False)\n",
    "rfc_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\rfc_pred.csv', index=False)\n",
    "smote_rfc_pred.to_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\smote_rfc_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our Pipeline model to a file\n",
    "joblib.dump(smote_rfc, 'D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\smote_rfc_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv\n",
    "df_results.to_csv('D:\\\\documentos\\\\IA Caballos\\\\hurdle_data\\\\class_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0544f3d8f88c277ece8a7bca9c4cae84dfc7832020b3e571294385a87fe1e90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
